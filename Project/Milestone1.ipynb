{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction in an Era of Unpredictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research questions\n",
    "\n",
    "To better dissect the data and extract insights, we will explore the following questions:\n",
    "\n",
    "1. Which socio-economic factors have the biggest impact, if any, in building or altering popular opinion?\n",
    "    * Standard and Quality of Education (Wikidata, Educational databases)\n",
    "    * Income and Inequality Levels (Wikidata)\n",
    "</br>\n",
    "\n",
    "2. How does media through newspapers and social networks influence these events through information or misinformation?\n",
    "    * Newspaper (Corpus Data)\n",
    "    * Social Media (Twitter)\n",
    "</br>\n",
    "\n",
    "3. What is the influence of the political system in place?\n",
    "    * Political System\n",
    "    * Rate of Abstention (Wikidata)\n",
    "    * Demonstrations Covered in Media (Corpus Data)\n",
    "</br>\n",
    "\n",
    "4. How far in time could we have traced the roots of such events? (Wikidata, Newspapers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which socio-economic factors have the biggest impact, if any, in building or altering popular opinion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard and Quality of Education (Wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edu_df = pd.DataFrame(columns=['Country',\n",
    "                               'Total Literacy',\n",
    "                               'Secondary Diploma',\n",
    "                               'Post-Secondary Diploma',\n",
    "                               'Total Enrolled',\n",
    "                               'National Education Budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edu_df.loc[len(edu_df)] = ['United States',\n",
    "                           '100%',\n",
    "                           '81%',\n",
    "                           '30%',\n",
    "                           '81.5 million',\n",
    "                           '1 trillion']\n",
    "\n",
    "edu_df.loc[len(edu_df)] = ['United Kingdom',\n",
    "                           '99%',\n",
    "                           '87%',\n",
    "                           '41%',\n",
    "                           '11.7 million',\n",
    "                           '62.2 billion']\n",
    "\n",
    "edu_df.loc[len(edu_df)] = ['Spain',\n",
    "                           '98.1%',\n",
    "                           '45.4%',\n",
    "                           '38.1%',\n",
    "                           '5.9 million',\n",
    "                           '4.04 billion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total Literacy</th>\n",
       "      <th>Secondary Diploma</th>\n",
       "      <th>Post-Secondary Diploma</th>\n",
       "      <th>Total Enrolled</th>\n",
       "      <th>National Education Budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>100%</td>\n",
       "      <td>81%</td>\n",
       "      <td>30%</td>\n",
       "      <td>81.5 million</td>\n",
       "      <td>1 trillion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>99%</td>\n",
       "      <td>87%</td>\n",
       "      <td>41%</td>\n",
       "      <td>11.7 million</td>\n",
       "      <td>62.2 billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spain</td>\n",
       "      <td>98.1%</td>\n",
       "      <td>45.4%</td>\n",
       "      <td>38.1%</td>\n",
       "      <td>5.9 million</td>\n",
       "      <td>4.04 billion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country Total Literacy Secondary Diploma Post-Secondary Diploma  \\\n",
       "0   United States           100%               81%                    30%   \n",
       "1  United Kingdom            99%               87%                    41%   \n",
       "2           Spain          98.1%             45.4%                  38.1%   \n",
       "\n",
       "  Total Enrolled National Education Budget  \n",
       "0   81.5 million                1 trillion  \n",
       "1   11.7 million              62.2 billion  \n",
       "2    5.9 million              4.04 billion  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- https://en.wikipedia.org/wiki/Education_in_the_United_States\n",
    "- https://en.wikipedia.org/wiki/Education_in_England\n",
    "- https://en.wikipedia.org/wiki/List_of_Ministers_of_Education_of_Spain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income and Inequality Levels (Wikidata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does media through newspapers and social networks influence these events through information or misinformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, we will focus on newspaper articles from the United States, United Kingdom, and Spain from the News On The Web Corpus Dataset. In terms of data collection, we will rely on articles regarding the U.S 2016 Presidential Election."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the appropriate articles, we iterate through all of the articles from 2016 within the corpus and only select those with key terms relevant to this topic (i.e. 'election', 'Trump', etc). Once we have these relevant articles, we convert the text to lowercase, remove the punctuation so words are not tokenized with the punctuation marks, remove common stopwords from the text, and then find the most important or trending words using the method described below.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Term Frequency (TF) and Inverse Document Frequency (IDF) methods as discussed in class and the previous homework.\n",
    "- TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "- IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term \"T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the appropriate articles about the election, we will compute the TF - IDF features to find words that score the highest in this set of articles.\n",
    "\n",
    "Later, when scaling to the full dataset on the cluster, we will select the top 20 words for each article and then use clustering techniques to find articles with close word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE_1_PATH = \"./Data/now_sources_pt1.txt\"\n",
    "SOURCE_2_PATH = \"./Data/now_sources_pt2.txt\"\n",
    "DATA_PATH = \"./Data/16-10-us.txt\"\n",
    "ENCODING = \"latin1\"\n",
    "\n",
    "COLUMN_NAMES = ['TextID',\n",
    "                'Number of Words',\n",
    "                'Date',\n",
    "                'Country',\n",
    "                'Source',\n",
    "                'Link',\n",
    "                'Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextID</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1334669</td>\n",
       "      <td>334</td>\n",
       "      <td>10-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>The Next Web</td>\n",
       "      <td>http://thenextweb.com/2010/01/01/avatar-takes-...</td>\n",
       "      <td>Believe it or not: Avatar takes 1 petabyte of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1334671</td>\n",
       "      <td>493</td>\n",
       "      <td>10-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>People Magazine</td>\n",
       "      <td>http://www.people.com/people/article/0,,203339...</td>\n",
       "      <td>INSIDE STORY: The Making of Beyonc's 'Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1334672</td>\n",
       "      <td>1255</td>\n",
       "      <td>10-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>San Francisco Chronicle</td>\n",
       "      <td>http://www.sfgate.com/bayarea/article/Biblical...</td>\n",
       "      <td>Biblical scholar's date for rapture: May 21, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1334673</td>\n",
       "      <td>695</td>\n",
       "      <td>10-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://www.cnn.com/2010/HEALTH/01/01/multi.vit...</td>\n",
       "      <td>What you need to know about multivitamins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1334674</td>\n",
       "      <td>724</td>\n",
       "      <td>10-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>MedPage Today</td>\n",
       "      <td>http://www.medpagetoday.com/Psychiatry/sleepdi...</td>\n",
       "      <td>Lack of Sleep Linked to Depression in Adolescents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TextID  Number of Words      Date Country                   Source  \\\n",
       "0  1334669              334  10-01-01      US             The Next Web   \n",
       "1  1334671              493  10-01-01      US          People Magazine   \n",
       "2  1334672             1255  10-01-01      US  San Francisco Chronicle   \n",
       "3  1334673              695  10-01-01      US                      CNN   \n",
       "4  1334674              724  10-01-01      US            MedPage Today   \n",
       "\n",
       "                                                Link  \\\n",
       "0  http://thenextweb.com/2010/01/01/avatar-takes-...   \n",
       "1  http://www.people.com/people/article/0,,203339...   \n",
       "2  http://www.sfgate.com/bayarea/article/Biblical...   \n",
       "3  http://www.cnn.com/2010/HEALTH/01/01/multi.vit...   \n",
       "4  http://www.medpagetoday.com/Psychiatry/sleepdi...   \n",
       "\n",
       "                                            Headline  \n",
       "0  Believe it or not: Avatar takes 1 petabyte of ...  \n",
       "1  INSIDE STORY: The Making of Beyonc's 'Single ...  \n",
       "2  Biblical scholar's date for rapture: May 21, 2011  \n",
       "3          What you need to know about multivitamins  \n",
       "4  Lack of Sleep Linked to Depression in Adolescents  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open(SOURCE_1_PATH, \"r\", encoding=ENCODING) as f:\n",
    "    pt1 = pd.read_csv(f, sep=\"\\t\", names = COLUMN_NAMES)\n",
    "    \n",
    "pt1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextID</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2930853</td>\n",
       "      <td>194</td>\n",
       "      <td>15-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>Consequence of Sound</td>\n",
       "      <td>http://consequenceofsound.net/2015/01/100000-c...</td>\n",
       "      <td>100000 copies of The Interview will be dropped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2930854</td>\n",
       "      <td>266</td>\n",
       "      <td>15-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>http://www.latimes.com/entertainment/movies/la...</td>\n",
       "      <td>'The Taking of Tiger Mountain' gets blockbuste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2930855</td>\n",
       "      <td>701</td>\n",
       "      <td>15-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>TIME</td>\n",
       "      <td>http://time.com/3651697/afghanistan-war-cost/</td>\n",
       "      <td>The True Cost of the Afghanistan War May Surpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2930856</td>\n",
       "      <td>1963</td>\n",
       "      <td>15-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>The Week Magazine</td>\n",
       "      <td>http://theweek.com/articles/441310/confessions...</td>\n",
       "      <td>Confessions of a former TSA officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2930859</td>\n",
       "      <td>263</td>\n",
       "      <td>15-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>Russia Beyond the Headlines</td>\n",
       "      <td>http://rbth.com/arts/2015/01/01/year_of_litera...</td>\n",
       "      <td>Pushkin, Gogol and Akhmatova to be symbols of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TextID  Number of Words      Date Country                       Source  \\\n",
       "0  2930853              194  15-01-01      US         Consequence of Sound   \n",
       "1  2930854              266  15-01-01      US            Los Angeles Times   \n",
       "2  2930855              701  15-01-01      US                         TIME   \n",
       "3  2930856             1963  15-01-01      US            The Week Magazine   \n",
       "4  2930859              263  15-01-01      US  Russia Beyond the Headlines   \n",
       "\n",
       "                                                Link  \\\n",
       "0  http://consequenceofsound.net/2015/01/100000-c...   \n",
       "1  http://www.latimes.com/entertainment/movies/la...   \n",
       "2      http://time.com/3651697/afghanistan-war-cost/   \n",
       "3  http://theweek.com/articles/441310/confessions...   \n",
       "4  http://rbth.com/arts/2015/01/01/year_of_litera...   \n",
       "\n",
       "                                            Headline  \n",
       "0  100000 copies of The Interview will be dropped...  \n",
       "1  'The Taking of Tiger Mountain' gets blockbuste...  \n",
       "2  The True Cost of the Afghanistan War May Surpr...  \n",
       "3                Confessions of a former TSA officer  \n",
       "4  Pushkin, Gogol and Akhmatova to be symbols of ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open(SOURCE_2_PATH, \"r\", encoding=\"latin1\") as f:\n",
    "    pt2 = pd.read_csv(f, sep=\"\\t\", names = COLUMN_NAMES)\n",
    "    \n",
    "pt2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newspaper Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@14637197 &lt;p&gt; NEW YORK ( AP ) -- Donald Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@14637200 &lt;h&gt; Here 's all the crazy stuff tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@14637201 &lt;h&gt; Another hotel wants to build in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@14637202 &lt;p&gt; In this Sept. 23 , 2016 photo ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@14637203 &lt;h&gt; Court erases jury award of puni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@@14637204 &lt;p&gt; A flurry of small earthquakes n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@@14637207 &lt;h&gt; After a dominant win over Stanf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@@14637209 &lt;h&gt; Story Highlights &lt;p&gt; A dramatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@@14637213 &lt;h&gt; Hueneme suffers first loss of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@@14637215 &lt;p&gt; CLOVERDALE -- A Napoleon woman ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "0  @@14637197 <p> NEW YORK ( AP ) -- Donald Trump...\n",
       "1  @@14637200 <h> Here 's all the crazy stuff tha...\n",
       "2  @@14637201 <h> Another hotel wants to build in...\n",
       "3  @@14637202 <p> In this Sept. 23 , 2016 photo ,...\n",
       "4  @@14637203 <h> Court erases jury award of puni...\n",
       "5  @@14637204 <p> A flurry of small earthquakes n...\n",
       "6  @@14637207 <h> After a dominant win over Stanf...\n",
       "7  @@14637209 <h> Story Highlights <p> A dramatic...\n",
       "8  @@14637213 <h> Hueneme suffers first loss of s...\n",
       "9  @@14637215 <p> CLOVERDALE -- A Napoleon woman ..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the lines in the file\n",
    "with codecs.open(DATA_PATH, \"r\", encoding=ENCODING) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# list to store all the lines read from the data file\n",
    "data = []\n",
    "\n",
    "# append all the lines to the list called data\n",
    "for line in lines:\n",
    "    data.append(line)\n",
    "\n",
    "# create DataFrame using the list of lines\n",
    "df = pd.DataFrame(data, columns=[\"Content\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all the punctuations in the text with a space. We first tried to replace it with an empty sequence but that replaces words such as \"Trump's\" with \"Trumps\". More importantly, finding all the relevant keywords using a regex also selects articles with words \"trumpets\", \"selection\", etc. which have the words \"trump\", \"election\" as substrings. So, we try to find the words \"trump \", \"election \", both ending with a space. So we need to replace punctuations with a space to find those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Content\n",
      "0    14637197  p  new york   ap      donald trump...\n",
      "1    14637200  h  here  s all the crazy stuff tha...\n",
      "2    14637201  h  another hotel wants to build in...\n",
      "3    14637202  p  in this sept  23   2016 photo  ...\n",
      "4    14637203  h  court erases jury award of puni...\n",
      "5    14637204  p  a flurry of small earthquakes n...\n",
      "6    14637207  h  after a dominant win over stanf...\n",
      "7    14637209  h  story highlights  p  a dramatic...\n",
      "8    14637213  h  hueneme suffers first loss of s...\n",
      "9    14637215  p  cloverdale    a napoleon woman ...\n",
      "\n",
      "Number of articles = 1000\n"
     ]
    }
   ],
   "source": [
    "# function to replace all the punctuations with a space\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, \" \")\n",
    "    return text\n",
    "\n",
    "# convert all text to lowercase\n",
    "df[\"Content\"] = df[\"Content\"].str.lower()\n",
    "\n",
    "# apply function to the given column\n",
    "df[\"Content\"] = df[\"Content\"].apply(remove_punctuations)\n",
    "\n",
    "print(df.head(10))\n",
    "print(\"\\nNumber of articles = {}\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Content\n",
      "0     14637197  p  new york   ap      donald trump...\n",
      "10    14637217  h  utility  s rates   old market f...\n",
      "44    14638417  p  file   in this june 5   2015   ...\n",
      "45    14638418  p  iowa city    a powerful univers...\n",
      "54    14638430  h  tired of the rancor of presiden...\n",
      "77    14639013  h  start listening to audiobooks w...\n",
      "81    14639017  h  with the death of shimon peres ...\n",
      "87    14639582  p  wbns tv  s on line public inspe...\n",
      "92    14639591  h  multiple fresh signs of economi...\n",
      "95    14639594  p  a new president will take the o...\n",
      "\n",
      "Number of articles = 116\n"
     ]
    }
   ],
   "source": [
    "# to store all the articles relevant to our project from the file\n",
    "relevant_keywords = [\"election\",\n",
    "                     \"trump\",\n",
    "                     \"donald\"\n",
    "                     \"clinton\",\n",
    "                     \"hillary\"]\n",
    "\n",
    "# NOTE: add a space between the OR | symbol between the words so\n",
    "# we do not choose words which have relevant words as substrings\n",
    "relevant_regex = \" |\".join(relevant_keywords)\n",
    "\n",
    "# find all articles which contain any of the relevant keywords\n",
    "df = df[df[\"Content\"].str.contains(relevant_regex)]\n",
    "\n",
    "print(df.head(10))\n",
    "print(\"\\nNumber of articles = {}\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most important and trending words using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding trending words in 116 articles\n",
      "\n",
      "Converted text into vectors of shape (116, 10161)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9447</th>\n",
       "      <td>trump</td>\n",
       "      <td>0.071417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>said</td>\n",
       "      <td>0.041611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>clinton</td>\n",
       "      <td>0.035538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9073</th>\n",
       "      <td>tax</td>\n",
       "      <td>0.031292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>election</td>\n",
       "      <td>0.024458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>donald</td>\n",
       "      <td>0.021548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>new</td>\n",
       "      <td>0.020607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>campaign</td>\n",
       "      <td>0.020297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9076</th>\n",
       "      <td>taxes</td>\n",
       "      <td>0.020268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>times</td>\n",
       "      <td>0.020098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>presidential</td>\n",
       "      <td>0.019985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>party</td>\n",
       "      <td>0.019375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>hillary</td>\n",
       "      <td>0.019044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>people</td>\n",
       "      <td>0.018459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>year</td>\n",
       "      <td>0.016688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9840</th>\n",
       "      <td>vote</td>\n",
       "      <td>0.016611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9844</th>\n",
       "      <td>voters</td>\n",
       "      <td>0.015362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>state</td>\n",
       "      <td>0.015121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>just</td>\n",
       "      <td>0.015054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>day</td>\n",
       "      <td>0.015052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>million</td>\n",
       "      <td>0.014782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>years</td>\n",
       "      <td>0.014629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>like</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6731</th>\n",
       "      <td>percent</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>debate</td>\n",
       "      <td>0.014379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature     tfidf\n",
       "9447          trump  0.071417\n",
       "8013           said  0.041611\n",
       "1938        clinton  0.035538\n",
       "9073            tax  0.031292\n",
       "3203       election  0.024458\n",
       "3004         donald  0.021548\n",
       "6190            new  0.020607\n",
       "1598       campaign  0.020297\n",
       "9076          taxes  0.020268\n",
       "9252          times  0.020098\n",
       "7089   presidential  0.019985\n",
       "6638          party  0.019375\n",
       "4461        hillary  0.019044\n",
       "6725         people  0.018459\n",
       "10122          year  0.016688\n",
       "9840           vote  0.016611\n",
       "9844         voters  0.015362\n",
       "8697          state  0.015121\n",
       "5140           just  0.015054\n",
       "2562            day  0.015052\n",
       "5885        million  0.014782\n",
       "10124         years  0.014629\n",
       "5440           like  0.014546\n",
       "6731        percent  0.014511\n",
       "2582         debate  0.014379"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finding trending words in {} articles\\n\".format(df.shape[0]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "vectors = vectorizer.fit_transform(df.Content)\n",
    "print(\"Converted text into vectors of shape {}\".format(vectors.shape))\n",
    "\n",
    "weights = np.asarray(vectors.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({\n",
    "                \"feature\": vectorizer.get_feature_names(),\n",
    "                \"tfidf\": weights\n",
    "            })\n",
    "\n",
    "sorted_df = weights_df.sort_values(by=\"tfidf\", ascending=False)\n",
    "sorted_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "ax = sns.barplot(\n",
    "        x=\"feature\",\n",
    "        y=\"tfidf\",\n",
    "        data=sorted_df)\n",
    "\n",
    "ax.set_title(\"Top 25 Features\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
